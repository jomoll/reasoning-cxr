{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "99bfab0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import transformers \n",
    "import torch\n",
    "import yaml\n",
    "from shutil import copyfile\n",
    "from tqdm import tqdm \n",
    "from openai import AzureOpenAI\n",
    "import random\n",
    "import json\n",
    "from datasets import load_dataset, Dataset, DatasetDict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "056546e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "metadata_path = \"data/keno_1000/Metadata_1000_only_new.csv\"\n",
    "output_dir = \"data/keno_1000/vqa\"\n",
    "model_name = \"jb-turbo-2024-04-09\"  \n",
    "\n",
    "\n",
    "client = AzureOpenAI(\n",
    "    api_key=\"e849b8c4c4a04d3d817aa67d66189251\",\n",
    "    api_version=\"2024-02-01\",\n",
    "    azure_endpoint=\"https://jb-turbo-2024-04-09.openai.azure.com/\",\n",
    ")\n",
    "\n",
    "# Replace the iteration section\n",
    "total_images = 1000  # Number of images to process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "031007d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num question types: 23\n"
     ]
    }
   ],
   "source": [
    "question_types = [\n",
    "    \"Does the chest X-ray show cardiomegaly (yes or no)?\",\n",
    "    \"Is there a pulmonary congestion (yes or no)?\",\n",
    "    \"Is there a right-sided pleural effusion (yes or no)?\",\n",
    "    \"Is there a left-sided pleural effusion (yes or no)?\",\n",
    "    \"Are there opacities in the right lung (yes or no)?\",\n",
    "    \"Are there opacities in the left lung (yes or no)?\",\n",
    "    \"Is there a right-sided atelectasis (yes or no)?\",\n",
    "    \"Is there a left-sided atelectasis (yes or no)?\",\n",
    "    \"Is there a right-sided pneumothorax (yes or no)?\",\n",
    "    \"Is there a left-sided pneumothorax (yes or no)?\",\n",
    "    \"Is there a central venous catheter present in the image (yes or no)?\",\n",
    "    \"Is there a gastric tube present in the image (yes or no)?\",\n",
    "\n",
    "    \"What is the size of the heart (normal, borderline, enlarged, massively enlarged)?\",\n",
    "    \"What is the severity of pulmonary congestion (none, questionable, mild, moderate, severe)?\",\n",
    "    \"What is the severity of the pleural effusion on the right (none, questionable, mild, moderate, severe)?\",\n",
    "    \"What is the severity of the pleural effusion on the left (none, questionable, mild, moderate, severe)?\",\n",
    "    \"What is the severity of right-sided pulmonary opacities (none, questionable, mild, moderate, severe)?\",\n",
    "    \"What is the severity of left-sided pulmonary opacities (none, questionable, mild, moderate, severe)?\",\n",
    "    \"What is the severity of right-sided atelectasis (none, questionable, mild, moderate, severe)?\",\n",
    "    \"What is the severity of left-sided atelectasis (none, questionable, mild, moderate, severe)?\",\n",
    "\n",
    "    \"Which side has worse pleural effusion? (right, left, same severity, absent)?\",\n",
    "    \"Which side has worse pulmonary opacities? (right, left, same severity, absent)?\",\n",
    "    \"Which side has worse atelectasis? (right, left, same severity, absent)?\",\n",
    "]\n",
    "print(\"Num question types:\", len(question_types))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0cf9ab3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "metadata_df = pd.read_csv(metadata_path)\n",
    "metadata_df.set_index(\"UID\", inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "e863506c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 637 existing VQA samples from data/keno_1000/vqa\n",
      "Processed 110/1000 images\n",
      "Processed 120/1000 images\n",
      "Processed 130/1000 images\n",
      "Processed 140/1000 images\n",
      "Processed 150/1000 images\n",
      "Processed 160/1000 images\n",
      "Processed 170/1000 images\n",
      "Processed 180/1000 images\n",
      "Processed 190/1000 images\n",
      "Processed 200/1000 images\n",
      "Processed 210/1000 images\n",
      "Processed 220/1000 images\n",
      "Processed 230/1000 images\n",
      "Processed 240/1000 images\n",
      "Processed 250/1000 images\n",
      "Processed 260/1000 images\n",
      "Processed 270/1000 images\n",
      "Processed 280/1000 images\n",
      "Processed 290/1000 images\n",
      "Processed 300/1000 images\n",
      "Processed 310/1000 images\n",
      "Processed 320/1000 images\n",
      "Processed 330/1000 images\n",
      "Processed 340/1000 images\n",
      "Processed 350/1000 images\n",
      "Processed 360/1000 images\n",
      "Processed 370/1000 images\n",
      "Processed 380/1000 images\n",
      "Processed 390/1000 images\n",
      "Processed 400/1000 images\n",
      "Processed 410/1000 images\n",
      "Processed 420/1000 images\n",
      "Processed 430/1000 images\n",
      "Processed 440/1000 images\n",
      "Processed 450/1000 images\n",
      "Processed 460/1000 images\n",
      "Processed 470/1000 images\n",
      "Processed 480/1000 images\n",
      "Processed 490/1000 images\n",
      "Processed 500/1000 images\n",
      "Processed 510/1000 images\n",
      "Processed 520/1000 images\n",
      "Processed 530/1000 images\n",
      "Processed 540/1000 images\n",
      "Processed 550/1000 images\n",
      "Processed 560/1000 images\n",
      "Processed 570/1000 images\n",
      "Processed 580/1000 images\n",
      "Processed 590/1000 images\n",
      "Processed 600/1000 images\n",
      "Processed 610/1000 images\n",
      "Processed 620/1000 images\n",
      "Processed 630/1000 images\n",
      "Processed 640/1000 images\n",
      "Processed 650/1000 images\n",
      "Processed 660/1000 images\n",
      "Processed 670/1000 images\n",
      "Processed 680/1000 images\n",
      "Processed 690/1000 images\n",
      "Processed 700/1000 images\n",
      "Processed 710/1000 images\n",
      "Processed 720/1000 images\n",
      "Processed 730/1000 images\n",
      "Processed 740/1000 images\n",
      "Processed 750/1000 images\n",
      "Processed 760/1000 images\n",
      "Processed 770/1000 images\n",
      "Processed 780/1000 images\n",
      "Processed 790/1000 images\n",
      "Processed 800/1000 images\n",
      "Processed 810/1000 images\n",
      "Processed 820/1000 images\n",
      "Processed 830/1000 images\n",
      "Processed 840/1000 images\n",
      "Processed 850/1000 images\n",
      "Processed 860/1000 images\n",
      "Processed 870/1000 images\n",
      "Processed 880/1000 images\n",
      "Processed 890/1000 images\n",
      "Processed 900/1000 images\n",
      "Processed 910/1000 images\n",
      "Processed 920/1000 images\n",
      "Processed 930/1000 images\n",
      "Processed 940/1000 images\n",
      "Processed 950/1000 images\n",
      "Processed 960/1000 images\n",
      "Processed 970/1000 images\n",
      "Processed 980/1000 images\n",
      "Processed 990/1000 images\n",
      "Processed 1000/1000 images\n",
      "Generated 6364 VQA samples\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'image_id': 'd4507d8df405e051cdbbcc9d07ef5303438b26938fd35e8960ed96a0271b8bf6',\n",
       "  'Question': 'Is there a pulmonary congestion (yes or no)?',\n",
       "  'Answer': 'yes',\n",
       "  'Type': 'binary',\n",
       "  'PatientID': 'b9ad4fa9-2312-4706-8c15-ba962861eca7',\n",
       "  'Age': 24124,\n",
       "  'Cardiomegaly': 1,\n",
       "  'PulmonaryCongestion': 2,\n",
       "  'PleuralEffusion_Right': 3,\n",
       "  'PleuralEffusion_Left': 2,\n",
       "  'PulmonaryOpacities_Right': 3,\n",
       "  'PulmonaryOpacities_Left': 0,\n",
       "  'Atelectasis_Right': 2,\n",
       "  'Atelectasis_Left': 1,\n",
       "  'Pneumothorax_Right': 0,\n",
       "  'Pneumothorax_Left': 0,\n",
       "  'Comments': 'Tracheostomy tubus in proper position. Gastric tube. '}]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "processed = 100\n",
    "cardio_map = {-1: \"not assessable\", 0: \"normal\", 1: \"borderline\", 2: \"enlarged\", 4: \"massively enlarged\"}\n",
    "other_map = {0: \"none\", 1: \"questionable\", 2: \"mild\", 3: \"moderate\", 4: \"severe\"}\n",
    "pneumo_map = {0: \"no\", 1: \"there is a\"}\n",
    "def describe_row(row):\n",
    "    parts = [f\"Patient age {int(row['Age'])//365} years\"]\n",
    "    parts.append(f\"{cardio_map.get(row['cardiomegaly2'], 'unknown')} heart size\")\n",
    "    parts.append(f\"{other_map.get(row['congestion2'], 'unknown')} congestion\")\n",
    "    parts.append(f\"{other_map.get(row['pleural_effusion_right2'], 'unknown')} right pulmonary opacities\")\n",
    "    parts.append(f\"{other_map.get(row['pleural_effusion_left2'], 'unknown')} left pulmonary opacities\")\n",
    "    parts.append(f\"{other_map.get(row['pneumonic_infiltrates_right2'], 'unknown')} right pneumonic infiltrates\")\n",
    "    parts.append(f\"{other_map.get(row['pneumonic_infiltrates_left2'], 'unknown')} left pneumonic infiltrates\")\n",
    "    parts.append(f\"{other_map.get(row['atelectasis_right2'], 'unknown')} right atelectasis\")\n",
    "    parts.append(f\"{other_map.get(row['atelectasis_left2'], 'unknown')} left atelectasis\")\n",
    "    parts.append(f\"{pneumo_map.get(row['pneumothorax_right'], 'unknown')} right pneumothorax\")\n",
    "    parts.append(f\"{pneumo_map.get(row['pneumothorax_left'], 'unknown')} left pneumothorax\")\n",
    "    parts.append(f\"{(row['Sonstiges'], 'unknown')}\")\n",
    "    return \"Clinical data: \" + \", \".join(parts) + \".\"\n",
    "\n",
    "def generate_answer(question, row):\n",
    "    clinical_info = describe_row(row)\n",
    "    if not clinical_info:\n",
    "        return \"No clinical information available for this image.\"\n",
    "    prompt = f\"Given the following clinical information, answer the question. Assume support devices not mentioned as absent: {clinical_info}\\nQuestion: {question}\\nAnswer:\"\n",
    "    response = client.chat.completions.create(\n",
    "        model=model_name,\n",
    "        messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "        max_tokens=50,\n",
    "        temperature=0.0,\n",
    "    )\n",
    "    answer = response.choices[0].message.content.strip()\n",
    "    return answer\n",
    "# Sample annotation generation script\n",
    "def generate_vqa_pairs(metadata_df, question_types, num_questions_per_image=5):\n",
    "    # check if output file exists and if yes, load it\n",
    "    if not os.path.exists(output_dir):\n",
    "        os.makedirs(output_dir)\n",
    "    if os.path.exists(os.path.join(output_dir, \"vqa_eval_set.jsonl\")):\n",
    "        with open(os.path.join(output_dir, \"vqa_eval_set.jsonl\"), \"r\") as f:\n",
    "            vqa_data = [json.loads(line) for line in f]\n",
    "        print(f\"Loaded {len(vqa_data)} existing VQA samples from {output_dir}\")\n",
    "    else:\n",
    "        vqa_data = []\n",
    "    for uid, row in metadata_df.iterrows():\n",
    "        # check if uid already exists in vqa_data\n",
    "        if any(sample['image_id'] == uid for sample in vqa_data):\n",
    "            continue\n",
    "        selected_questions = random.sample(question_types, num_questions_per_image)\n",
    "        for question in selected_questions:\n",
    "            answer = generate_answer(question, row)\n",
    "            question_type = question.split(\" \")[0]  # Extract the type of question (e.g., \"Does\", \"Is\", \"What\")\n",
    "            if question_type.lower() in [\"does\", \"is\", \"are\"]:\n",
    "                question_type = \"binary\"\n",
    "                if answer.lower() in [\"yes\", \"no\"]:\n",
    "                    answer = \"yes\" if \"yes\" in answer.lower() else \"no\"\n",
    "                # else discard the answer if it doesn't match expected binary response and try another question\n",
    "                else:\n",
    "                    continue\n",
    "            elif question_type.lower() in [\"what\"]:\n",
    "                question_type = \"ordinal\"\n",
    "                if answer.lower() in [\"none\", \"questionable\", \"mild\", \"moderate\", \"severe\", \"normal\", \"borderline\", \"enlarged\", \"massively enlarged\"]:\n",
    "                    answer = answer.lower()\n",
    "                else:\n",
    "                    continue\n",
    "            else:\n",
    "                question_type = \"comparison\"\n",
    "                if answer.lower() in [\"right\", \"left\", \"same severity\", \"absent\"]:\n",
    "                    answer = answer.lower()\n",
    "                else:\n",
    "                    continue\n",
    "            vqa_sample = {\n",
    "                \"image_id\": uid,\n",
    "                \"Question\": question,\n",
    "                \"Answer\": answer,  \n",
    "                \"Type\": question_type,\n",
    "                \"PatientID\": row['PatientID'],\n",
    "                \"Age\": row['Age'],\n",
    "                \"Cardiomegaly\": row['cardiomegaly2'],\n",
    "                \"PulmonaryCongestion\": row['congestion2'],\n",
    "                \"PleuralEffusion_Right\": row['pleural_effusion_right2'],\n",
    "                \"PleuralEffusion_Left\": row['pleural_effusion_left2'],\n",
    "                \"PulmonaryOpacities_Right\": row['pneumonic_infiltrates_right2'],\n",
    "                \"PulmonaryOpacities_Left\": row['pneumonic_infiltrates_left2'],\n",
    "                \"Atelectasis_Right\": row['atelectasis_right2'],\n",
    "                \"Atelectasis_Left\": row['atelectasis_left2'],\n",
    "                \"Pneumothorax_Right\": row['pneumothorax_right'],\n",
    "                \"Pneumothorax_Left\": row['pneumothorax_left'],\n",
    "                \"Comments\": row['Sonstiges'],\n",
    "            }\n",
    "            vqa_data.append(vqa_sample)\n",
    "        global processed\n",
    "        processed += 1\n",
    "        if processed % 10 == 0:\n",
    "            print(f\"Processed {processed}/{total_images} images\")\n",
    "        if processed >= total_images:\n",
    "            break\n",
    "    print(f\"Generated {len(vqa_data)} VQA samples\")\n",
    "\n",
    "    return vqa_data\n",
    "\n",
    "# Generate samples\n",
    "vqa_samples = generate_vqa_pairs(metadata_df, question_types, num_questions_per_image=8)\n",
    "\n",
    "# Save to JSONL\n",
    "output_file = os.path.join(output_dir, \"vqa_eval_set.jsonl\")\n",
    "with open(output_file, \"w\") as f:\n",
    "    for entry in vqa_samples:\n",
    "        f.write(json.dumps(entry) + \"\\n\")\n",
    "\n",
    "# Display sample\n",
    "vqa_samples[:1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "4a23731f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated 6364 VQA samples and saved to data/keno_1000/vqa/vqa_eval_set.jsonl\n",
      "Dataset({\n",
      "    features: ['UID', 'Question', 'Answer', 'Type', 'PatientID', 'Age', 'HeartSize', 'PulmonaryCongestion', 'PleuralEffusion_Right', 'PleuralEffusion_Left', 'PulmonaryOpacities_Right', 'PulmonaryOpacities_Left', 'Atelectasis_Right', 'Atelectasis_Left'],\n",
      "    num_rows: 6364\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "print(f\"Generated {len(vqa_samples)} VQA samples and saved to {output_file}\")\n",
    "vqa_dict = {\n",
    "        'UID': [], 'Question': [], 'Answer': [], 'Type': [],\n",
    "        'PatientID': [], 'Age': [],\n",
    "        'HeartSize': [], 'PulmonaryCongestion': [],\n",
    "        'PleuralEffusion_Right': [], 'PleuralEffusion_Left': [],\n",
    "        'PulmonaryOpacities_Right': [], 'PulmonaryOpacities_Left': [],\n",
    "        'Atelectasis_Right': [], 'Atelectasis_Left': []\n",
    "    }\n",
    "for sample in vqa_samples:\n",
    "    vqa_dict['UID'].append(sample['image_id'])\n",
    "    vqa_dict['Question'].append(sample['Question'])\n",
    "    vqa_dict['Answer'].append(sample['Answer'])\n",
    "    vqa_dict['Type'].append(sample['Type'])\n",
    "    vqa_dict['PatientID'].append(sample['PatientID'])\n",
    "    vqa_dict['Age'].append(sample['Age'])\n",
    "    vqa_dict['HeartSize'].append(sample['Cardiomegaly'])\n",
    "    vqa_dict['PulmonaryCongestion'].append(sample['PulmonaryCongestion'])\n",
    "    vqa_dict['PleuralEffusion_Right'].append(sample['PleuralEffusion_Right'])\n",
    "    vqa_dict['PleuralEffusion_Left'].append(sample['PleuralEffusion_Left'])\n",
    "    vqa_dict['PulmonaryOpacities_Right'].append(sample['PulmonaryOpacities_Right'])\n",
    "    vqa_dict['PulmonaryOpacities_Left'].append(sample['PulmonaryOpacities_Left'])\n",
    "    vqa_dict['Atelectasis_Right'].append(sample['Atelectasis_Right'])\n",
    "    vqa_dict['Atelectasis_Left'].append(sample['Atelectasis_Left'])\n",
    "vqa_dataset = Dataset.from_dict(vqa_dict)\n",
    "print(vqa_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "2716b1b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Filter: 100%|██████████| 6364/6364 [00:02<00:00, 2417.25 examples/s]\n",
      "Map: 100%|██████████| 4071/4071 [01:04<00:00, 62.90 examples/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['UID', 'Question', 'Answer', 'Type', 'PatientID', 'Age', 'HeartSize', 'PulmonaryCongestion', 'PleuralEffusion_Right', 'PleuralEffusion_Left', 'PulmonaryOpacities_Right', 'PulmonaryOpacities_Left', 'Atelectasis_Right', 'Atelectasis_Left', 'Split', 'PhysicianID', 'StudyDate', 'Sex', 'Image'],\n",
      "    num_rows: 4071\n",
      "})\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Filter: 100%|██████████| 6364/6364 [00:00<00:00, 8069.06 examples/s]\n",
      "Map: 100%|██████████| 996/996 [00:08<00:00, 120.85 examples/s]\n",
      "Filter: 100%|██████████| 6364/6364 [00:00<00:00, 6754.52 examples/s]\n",
      "Map: 100%|██████████| 1297/1297 [00:13<00:00, 97.69 examples/s] \n"
     ]
    }
   ],
   "source": [
    "image_dataset = load_dataset(\"jomoll/TAIX-reasoning-v3.0-expert\", name=\"default\")\n",
    "\n",
    "# match the uids with the images\n",
    "def create_merged_dataset(split: str = \"train\") -> Dataset:\n",
    "    image_dataset_split = image_dataset[split]\n",
    "    # keep all vqa samples that have a matching uid in the image dataset\n",
    "    vqa_dataset_split = vqa_dataset.filter(lambda x: x['UID'] in image_dataset_split['UID'])\n",
    "    # enrich vqa dataset with images and metadata\n",
    "    uid2image = {row['UID']: row for row in image_dataset_split}\n",
    "\n",
    "    merged_dataset = vqa_dataset_split.map(\n",
    "        lambda x: {\n",
    "            \"Split\": uid2image[x['UID']]['Split'],\n",
    "            \"PhysicianID\": uid2image[x['UID']]['PhysicianID'],\n",
    "            \"StudyDate\": uid2image[x['UID']]['StudyDate'],\n",
    "            \"Sex\": uid2image[x['UID']][\"Sex\"],\n",
    "            \"Image\": uid2image[x['UID']]['Image'],\n",
    "        }\n",
    "    )\n",
    "    return merged_dataset\n",
    "\n",
    "vqa_dataset_train = create_merged_dataset(\"train\")\n",
    "print(vqa_dataset_train)\n",
    "vqa_dataset_val = create_merged_dataset(\"val\")\n",
    "vqa_dataset_test = create_merged_dataset(\"test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "8613f240",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 1357/1357 [00:00<00:00, 3454.35 examples/s]s]\n",
      "Creating parquet from Arrow format: 100%|██████████| 14/14 [00:00<00:00, 80.34ba/s]\n",
      "Map: 100%|██████████| 1357/1357 [00:00<00:00, 2515.56 examples/s] 3.83s/it]\n",
      "Creating parquet from Arrow format: 100%|██████████| 14/14 [00:00<00:00, 73.95ba/s]\n",
      "Map: 100%|██████████| 1357/1357 [00:00<00:00, 3423.50 examples/s] 4.27s/it]\n",
      "Creating parquet from Arrow format: 100%|██████████| 14/14 [00:00<00:00, 58.13ba/s]\n",
      "Uploading the dataset shards: 100%|██████████| 3/3 [00:12<00:00,  4.16s/it]\n",
      "Map: 100%|██████████| 996/996 [00:00<00:00, 4548.94 examples/s]t/s]\n",
      "Creating parquet from Arrow format: 100%|██████████| 10/10 [00:00<00:00, 84.79ba/s]\n",
      "Uploading the dataset shards: 100%|██████████| 1/1 [00:02<00:00,  2.72s/it]\n",
      "Map: 100%|██████████| 1297/1297 [00:00<00:00, 1787.79 examples/s]s]\n",
      "Creating parquet from Arrow format: 100%|██████████| 13/13 [00:00<00:00, 67.09ba/s]\n",
      "Uploading the dataset shards: 100%|██████████| 1/1 [00:03<00:00,  3.85s/it]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "CommitInfo(commit_url='https://huggingface.co/datasets/jomoll/TAIX-VQA/commit/be63dd210b54ea1c99ce9ddb2c648fe0f6672307', commit_message='Upload dataset', commit_description='', oid='be63dd210b54ea1c99ce9ddb2c648fe0f6672307', pr_url=None, repo_url=RepoUrl('https://huggingface.co/datasets/jomoll/TAIX-VQA', endpoint='https://huggingface.co', repo_type='dataset', repo_id='jomoll/TAIX-VQA'), pr_revision=None, pr_num=None)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_dict = DatasetDict({\n",
    "        'train': vqa_dataset_train,\n",
    "        'val': vqa_dataset_val,\n",
    "        'test': vqa_dataset_test\n",
    "    })\n",
    "\n",
    "dataset_dict.push_to_hub(\"jomoll/TAIX-VQA\", private=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "taix-ray",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
