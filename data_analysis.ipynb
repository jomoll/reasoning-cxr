{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c78a8221",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 10 common files present in all versions.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ HTML diff report generated: reasoning_diff_report.html\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import yaml\n",
    "import difflib\n",
    "from collections import defaultdict\n",
    "from pprint import pprint\n",
    "from IPython.display import HTML\n",
    "# ---------------------------------------------\n",
    "# STEP 1: Load YAML files\n",
    "# ---------------------------------------------\n",
    "\n",
    "# Set your paths\n",
    "base_path = 'data/keno_1000/annotations'  # <-- Change this\n",
    "version_dirs = ['v1.0', 'v1.1_Markus', 'v1.2', 'v1.3']\n",
    "\n",
    "# Build nested dictionary {filename: {version: yaml}}\n",
    "version_files = {}\n",
    "\n",
    "def load_yaml(filepath):\n",
    "    with open(filepath, 'r') as f:\n",
    "        return yaml.safe_load(f)\n",
    "\n",
    "for version in version_dirs:\n",
    "    version_path = os.path.join(base_path, version)\n",
    "    version_files[version] = set([\n",
    "        fname for fname in os.listdir(version_path) if fname.endswith('.yaml')\n",
    "    ])\n",
    "\n",
    "# Get intersection of filenames\n",
    "common_files = set.intersection(*version_files.values())\n",
    "print(f\"Found {len(common_files)} common files present in all versions.\")\n",
    "\n",
    "# Load YAML data only for common files\n",
    "data = defaultdict(dict)\n",
    "for version in version_dirs:\n",
    "    version_path = os.path.join(base_path, version)\n",
    "    for fname in common_files:\n",
    "        full_path = os.path.join(version_path, fname)\n",
    "        yaml_data = load_yaml(full_path)\n",
    "        data[fname][version] = yaml_data\n",
    "\n",
    "# ---------------------------------------------\n",
    "# STEP 2: Diff function per string\n",
    "# ---------------------------------------------\n",
    "\n",
    "def html_diff(a, b):\n",
    "    \"\"\"Return highlighted HTML diff between two strings.\"\"\"\n",
    "    if not a and not b:  # Both empty\n",
    "        return \"\"\n",
    "    if not a:  # First string is empty\n",
    "        return f'<span style=\"background-color: #a6f3a6\">{b}</span>'\n",
    "    if not b:  # Second string is empty\n",
    "        return f'<span style=\"background-color: #f3a6a6\">{a}</span>'\n",
    "    if a == b:  # Strings are identical\n",
    "        return b\n",
    "\n",
    "    # Split into words for better matching\n",
    "    def split_into_words(text):\n",
    "        return text.replace('\\n', ' \\n ').split()\n",
    "\n",
    "    words_a = split_into_words(a)\n",
    "    words_b = split_into_words(b)\n",
    "    \n",
    "    matcher = difflib.SequenceMatcher(None, words_a, words_b)\n",
    "    output = []\n",
    "    \n",
    "    for tag, i1, i2, j1, j2 in matcher.get_opcodes():\n",
    "        if tag == 'equal':\n",
    "            output.append(' '.join(words_b[j1:j2]))\n",
    "        elif tag == 'insert':\n",
    "            output.append(f'<span style=\"background-color: #a6f3a6\">{\" \".join(words_b[j1:j2])}</span>')\n",
    "        elif tag == 'delete':\n",
    "            output.append(f'<span style=\"background-color: #f3a6a6\">{\" \".join(words_a[i1:i2])}</span>')\n",
    "        elif tag == 'replace':\n",
    "            output.append(f'<span style=\"background-color: #f3e7a6\">{\" \".join(words_b[j1:j2])}</span>')\n",
    "    \n",
    "    # Rejoin text and fix newlines\n",
    "    return ' '.join(output).replace(' \\n ', '\\n')\n",
    "\n",
    "# ---------------------------------------------\n",
    "# STEP 3: Build HTML report\n",
    "# ---------------------------------------------\n",
    "\n",
    "# Update the html_output initialization with a legend section:\n",
    "html_output = \"\"\"\n",
    "<html><head>\n",
    "<style>\n",
    ".legend {\n",
    "    margin: 20px 0;\n",
    "    padding: 10px;\n",
    "    background-color: #f8f9fa;\n",
    "    border: 1px solid #ddd;\n",
    "}\n",
    ".legend-item {\n",
    "    display: inline-block;\n",
    "    margin-right: 20px;\n",
    "}\n",
    ".legend-color {\n",
    "    display: inline-block;\n",
    "    width: 20px;\n",
    "    height: 20px;\n",
    "    margin-right: 5px;\n",
    "    vertical-align: middle;\n",
    "}\n",
    "table { border-collapse: collapse; width: 100%; table-layout: fixed; }\n",
    "th, td { \n",
    "    border: 1px solid #999; \n",
    "    padding: 0.5rem; \n",
    "    vertical-align: top; \n",
    "    width: 25%;\n",
    "}\n",
    "th { background-color: #eee; }\n",
    "pre { \n",
    "    white-space: pre-wrap; \n",
    "    margin: 0;\n",
    "    word-wrap: break-word;\n",
    "}\n",
    "td:first-child { width: 10%; }\n",
    "td:not(:first-child) { width: 30%; }\n",
    ".search-container {\n",
    "    margin: 20px 0;\n",
    "    padding: 10px;\n",
    "    background-color: #f8f9fa;\n",
    "    border: 1px solid #ddd;\n",
    "    display: flex;\n",
    "    gap: 10px;\n",
    "    align-items: center;\n",
    "}\n",
    ".search-container input {\n",
    "    width: 300px;\n",
    "    padding: 8px;\n",
    "    font-size: 16px;\n",
    "    border: 1px solid #ddd;\n",
    "    border-radius: 4px;\n",
    "}\n",
    ".search-button {\n",
    "    padding: 8px 16px;\n",
    "    font-size: 16px;\n",
    "    background-color: #007bff;\n",
    "    color: white;\n",
    "    border: none;\n",
    "    border-radius: 4px;\n",
    "    cursor: pointer;\n",
    "}\n",
    ".search-button:hover {\n",
    "    background-color: #0056b3;\n",
    "}\n",
    ".report-section {\n",
    "    display: block;\n",
    "}\n",
    ".report-section.hidden {\n",
    "    display: none;\n",
    "}\n",
    "</style>\n",
    "<script>\n",
    "function filterUIDs() {\n",
    "    const searchTerm = document.getElementById('searchInput').value.toLowerCase();\n",
    "    const sections = document.getElementsByClassName('report-section');\n",
    "    \n",
    "    for (let section of sections) {\n",
    "        const uid = section.getAttribute('data-uid').toLowerCase();\n",
    "        if (uid.includes(searchTerm)) {\n",
    "            section.classList.remove('hidden');\n",
    "        } else {\n",
    "            section.classList.add('hidden');\n",
    "        }\n",
    "    }\n",
    "}\n",
    "</script>\n",
    "</head><body>\n",
    "\n",
    "<div class=\"legend\">\n",
    "    <div class=\"legend-item\">\n",
    "        <div class=\"legend-color\" style=\"background-color: #a6f3a6;\"></div>\n",
    "        <span>Additional content</span>\n",
    "    </div>\n",
    "    <div class=\"legend-item\">\n",
    "        <div class=\"legend-color\" style=\"background-color: #f3a6a6;\"></div>\n",
    "        <span>Missing content</span>\n",
    "    </div>\n",
    "    <div class=\"legend-item\">\n",
    "        <div class=\"legend-color\" style=\"background-color: #f3e7a6;\"></div>\n",
    "        <span>Changed content</span>\n",
    "    </div>\n",
    "</div>\n",
    "<div class=\"legend\">\n",
    "    <!-- ...existing legend items... -->\n",
    "</div>\n",
    "\n",
    "<div class=\"search-container\">\n",
    "    <input type=\"text\" \n",
    "           id=\"searchInput\" \n",
    "           placeholder=\"Search for UID...\" \n",
    "           autocomplete=\"off\">\n",
    "    <button class=\"search-button\" \n",
    "            onclick=\"filterUIDs()\">\n",
    "        Search\n",
    "    </button>\n",
    "</div>\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "# Update the section where we process each file\n",
    "for fname, versions in data.items():\n",
    "    # Start of report section\n",
    "    html_output += f'<div class=\"report-section\" data-uid=\"{fname}\">'\n",
    "    html_output += f'<h2>Image ID: {fname}</h2>'\n",
    "    \n",
    "    v1_0 = versions.get('v1.0')\n",
    "    base = versions.get('v1.1_Markus')\n",
    "    v1_2 = versions.get('v1.2')\n",
    "    v1_3 = versions.get('v1.3')\n",
    "\n",
    "    reasoning_v1_0 = v1_0.get('reasoning', {}).get('Reasoning', []) if v1_0 else []\n",
    "    reasoning_base = base.get('reasoning', {}).get('Reasoning', [])\n",
    "    reasoning_v1_2 = v1_2.get('reasoning', {}).get('Reasoning', []) if v1_2 else []\n",
    "    reasoning_v1_3 = v1_3.get('reasoning', {}).get('Reasoning', []) if v1_3 else []\n",
    "\n",
    "    max_steps = max(len(reasoning_v1_0), len(reasoning_base), len(reasoning_v1_2), len(reasoning_v1_3))\n",
    "\n",
    "    # Process all reasoning steps\n",
    "    for step in range(max_steps):\n",
    "        step_v1_0 = reasoning_v1_0[step] if step < len(reasoning_v1_0) else {}\n",
    "        step_base = reasoning_base[step] if step < len(reasoning_base) else {}\n",
    "        step_v1_2 = reasoning_v1_2[step] if step < len(reasoning_v1_2) else {}\n",
    "        step_v1_3 = reasoning_v1_3[step] if step < len(reasoning_v1_3) else {}\n",
    "\n",
    "        desc_v1_0 = step_v1_0.get('Description', '')\n",
    "        desc_base = step_base.get('Description', '')\n",
    "        desc_v1_2 = step_v1_2.get('Description', '')\n",
    "        desc_v1_3 = step_v1_3.get('Description', '')\n",
    "\n",
    "        action_v1_0 = '\\n'.join(step_v1_0.get('Action', []))\n",
    "        action_base = '\\n'.join(step_base.get('Action', []))\n",
    "        action_v1_2 = '\\n'.join(step_v1_2.get('Action', []))\n",
    "        action_v1_3 = '\\n'.join(step_v1_3.get('Action', []))\n",
    "\n",
    "\n",
    "        result_v1_0 = step_v1_0.get('Result', '')\n",
    "        result_base = step_base.get('Result', '')\n",
    "        result_v1_2 = step_v1_2.get('Result', '')\n",
    "        result_v1_3 = step_v1_3.get('Result', '')\n",
    "\n",
    "        # Add FinalAssessment if available\n",
    "        final_assessment_v1_0 = step_v1_0.get('FinalAssessment', '')\n",
    "        final_assessment_base = step_base.get('FinalAssessment', '')\n",
    "        final_assessment_v1_2 = step_v1_2.get('FinalAssessment', '')\n",
    "        final_assessment_v1_3 = step_v1_3.get('FinalAssessment', '')\n",
    "\n",
    "        html_output += '<table>'\n",
    "        html_output += f'<tr><th colspan=\"5\">Reasoning Step {step+1}</th></tr>'\n",
    "        html_output += '<tr><th>Section</th><th>v1.0 (Llama-3-8B)</th><th>v1.1 (Markus) (reference)</th><th>v1.2 (Llama-3-70B)</th><th>v1.3 (GPT4-Turbo)</th></tr>'\n",
    "\n",
    "        # Description\n",
    "        html_output += '<tr><td><b>Description</b></td>'\n",
    "        html_output += f'<td><pre>{html_diff(desc_base, desc_v1_0)}</pre></td>'  # Compare against base\n",
    "        html_output += f'<td><pre>{desc_base}</pre></td>'  # Base version\n",
    "        html_output += f'<td><pre>{html_diff(desc_base, desc_v1_2)}</pre></td>'\n",
    "        html_output += f'<td><pre>{html_diff(desc_base, desc_v1_3)}</pre></td></tr>'\n",
    "\n",
    "        # Action\n",
    "        html_output += '<tr><td><b>Action</b></td>'\n",
    "        html_output += f'<td><pre>{html_diff(action_base, action_v1_0)}</pre></td>'  # Compare against base\n",
    "        html_output += f'<td><pre>{action_base}</pre></td>'  # Base version\n",
    "        html_output += f'<td><pre>{html_diff(action_base, action_v1_2)}</pre></td>'\n",
    "        html_output += f'<td><pre>{html_diff(action_base, action_v1_3)}</pre></td></tr>'\n",
    "\n",
    "        # Result\n",
    "        html_output += '<tr><td><b>Result</b></td>'\n",
    "        html_output += f'<td><pre>{html_diff(result_base, result_v1_0)}</pre></td>'  # Compare against base\n",
    "        html_output += f'<td><pre>{result_base}</pre></td>'  # Base version\n",
    "        html_output += f'<td><pre>{html_diff(result_base, result_v1_2)}</pre></td>'\n",
    "        html_output += f'<td><pre>{html_diff(result_base, result_v1_3)}</pre></td></tr>'\n",
    "\n",
    "        html_output += '</table><br>'\n",
    "\n",
    "    # Add this after the reasoning steps loop but before closing the report-section div\n",
    "    html_output += '<table>'\n",
    "    html_output += '<tr><th colspan=\"5\">Final Assessment</th></tr>'\n",
    "    html_output += '<tr><th>Section</th><th>v1.0 (Llama-3-8B)</th><th>v1.1 (Markus) (reference)</th><th>v1.2 (Llama-3-70B)</th><th>v1.3 (GPT4-Turbo)</th></tr>'\n",
    "\n",
    "    # Get FinalAssessment from each version\n",
    "    final_v1_0 = v1_0.get('reasoning', {}).get('FinalAssessment', '') if v1_0 else ''\n",
    "    final_base = base.get('reasoning', {}).get('FinalAssessment', '')\n",
    "    final_v1_2 = v1_2.get('reasoning', {}).get('FinalAssessment', '') if v1_2 else ''\n",
    "    final_v1_3 = v1_3.get('reasoning', {}).get('FinalAssessment', '') if v1_3 else ''\n",
    "\n",
    "    # Format as string if list\n",
    "    if isinstance(final_v1_0, list): final_v1_0 = '\\n'.join(final_v1_0)\n",
    "    if isinstance(final_base, list): final_base = '\\n'.join(final_base)\n",
    "    if isinstance(final_v1_2, list): final_v1_2 = '\\n'.join(final_v1_2)\n",
    "    if isinstance(final_v1_3, list): final_v1_3 = '\\n'.join(final_v1_3)\n",
    "\n",
    "    html_output += '<tr><td><b>FinalAssessment</b></td>'\n",
    "    html_output += f'<td><pre>{html_diff(final_base, final_v1_0)}</pre></td>'\n",
    "    html_output += f'<td><pre>{final_base}</pre></td>'\n",
    "    html_output += f'<td><pre>{html_diff(final_base, final_v1_2)}</pre></td>'\n",
    "    html_output += f'<td><pre>{html_diff(final_base, final_v1_3)}</pre></td></tr>'\n",
    "\n",
    "    html_output += '</table><br>'\n",
    "    html_output += '</div>'  # Close report-section div\n",
    "\n",
    "# ---------------------------------------------\n",
    "# STEP 4: Save full HTML report\n",
    "# ---------------------------------------------\n",
    "\n",
    "output_path = 'reasoning_diff_report.html'\n",
    "with open(output_path, 'w') as f:\n",
    "    f.write(html_output)\n",
    "\n",
    "print(f\"✅ HTML diff report generated: {output_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "49a4e96b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import yaml\n",
    "import json\n",
    "from datasets import Dataset, DatasetDict\n",
    "import os\n",
    "from typing import Dict, List\n",
    "from tqdm import tqdm\n",
    "\n",
    "def load_yaml_file(filepath: str) -> Dict:\n",
    "    \"\"\"Load and validate a single YAML file.\"\"\"\n",
    "    try:\n",
    "        with open(filepath, 'r') as f:\n",
    "            data = yaml.safe_load(f)\n",
    "            \n",
    "        # Basic validation\n",
    "        required_fields = ['annotator', 'metadata', 'reasoning']\n",
    "        if not all(field in data for field in required_fields):\n",
    "            print(f\"Warning: Missing required fields in {filepath}\")\n",
    "            # skip this file\n",
    "            return {}\n",
    "        return data\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading {filepath}: {e}\")\n",
    "        return {}\n",
    "\n",
    "def process_yaml_files(folder_path: str) -> Dataset:\n",
    "    \"\"\"Process all YAML files in folder and create dataset.\"\"\"\n",
    "    data_dict = {\n",
    "        'UID': [], 'Annotator': [], 'Reasoning': [], 'FinalAssessment': [],\n",
    "        'Split': [], 'PatientID': [], 'PhysicianID': [], \n",
    "        'StudyDate': [], 'Age': [], 'Sex': [], \n",
    "        'HeartSize': [], 'PulmonaryCongestion': [],\n",
    "        'PleuralEffusion_Right': [], 'PleuralEffusion_Left': [],\n",
    "        'PulmonaryOpacities_Right': [], 'PulmonaryOpacities_Left': [],\n",
    "        'Atelectasis_Right': [], 'Atelectasis_Left': []\n",
    "    }\n",
    "    \n",
    "    yaml_files = [f for f in os.listdir(folder_path) if f.endswith('.yaml')]\n",
    "    \n",
    "    for yaml_file in tqdm(yaml_files, desc=\"Processing YAML files\"):\n",
    "        data = load_yaml_file(os.path.join(folder_path, yaml_file))\n",
    "        if not data or 'metadata' not in data:\n",
    "            continue\n",
    "        \n",
    "        # Extract metadata\n",
    "        metadata = data['metadata']\n",
    "        \n",
    "        data_dict['UID'].append(metadata.get('UID', ''))\n",
    "        data_dict['Annotator'].append(data.get('annotator', ''))\n",
    "        data_dict['Split'].append(metadata.get('Split', ''))\n",
    "        data_dict['PatientID'].append(metadata.get('PatientID', ''))\n",
    "        data_dict['PhysicianID'].append(metadata.get('PhysicianID', ''))\n",
    "        data_dict['StudyDate'].append(metadata.get('StudyDate', ''))\n",
    "        data_dict['Age'].append(metadata.get('Age', -1))\n",
    "        data_dict['Sex'].append(metadata.get('Sex', ''))\n",
    "        \n",
    "        # Extract findings\n",
    "        data_dict['HeartSize'].append(metadata.get('HeartSize', -1))\n",
    "        data_dict['PulmonaryCongestion'].append(metadata.get('PulmonaryCongestion', -1))\n",
    "        data_dict['PleuralEffusion_Right'].append(metadata.get('PleuralEffusion_Right', -1))\n",
    "        data_dict['PleuralEffusion_Left'].append(metadata.get('PleuralEffusion_Left', -1))\n",
    "        data_dict['PulmonaryOpacities_Right'].append(metadata.get('PulmonaryOpacities_Right', -1))\n",
    "        data_dict['PulmonaryOpacities_Left'].append(metadata.get('PulmonaryOpacities_Left', -1))\n",
    "        data_dict['Atelectasis_Right'].append(metadata.get('Atelectasis_Right', -1))\n",
    "        data_dict['Atelectasis_Left'].append(metadata.get('Atelectasis_Left', -1))\n",
    "        \n",
    "        # Extract reasoning - THIS IS WHERE THE FIX IS NEEDED\n",
    "        reasoning = data.get('reasoning', {})\n",
    "        \n",
    "        # Serialize reasoning to JSON strings to avoid mixed-type issues\n",
    "        reasoning_data = reasoning.get('Reasoning', [])\n",
    "        data_dict['Reasoning'].append(json.dumps(reasoning_data))\n",
    "        \n",
    "        final_assessment = reasoning.get('FinalAssessment', [])\n",
    "        data_dict['FinalAssessment'].append(json.dumps(final_assessment))\n",
    "\n",
    "    # Create dataset\n",
    "    dataset = Dataset.from_dict(data_dict)\n",
    "    \n",
    "    # Split into train/val/test\n",
    "    dataset_dict = DatasetDict({\n",
    "        'train': dataset.filter(lambda x: x['Split'] == 'train'),\n",
    "        'val': dataset.filter(lambda x: x['Split'] == 'val'),\n",
    "        'test': dataset.filter(lambda x: x['Split'] == 'test')\n",
    "    })\n",
    "    \n",
    "    print(f\"Total samples: {len(dataset)}\")\n",
    "    print(f\"Train: {len(dataset_dict['train'])}\")\n",
    "    print(f\"val: {len(dataset_dict['val'])}\")\n",
    "    print(f\"Test: {len(dataset_dict['test'])}\")\n",
    "    \n",
    "    return dataset_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9109ff41",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing YAML files:  17%|█▋        | 657/3977 [00:08<00:42, 77.69it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: Missing required fields in data/keno_1000/annotations/v2.0/7d615a90487868f7db68e21dbc90555b82a319642bf55e0fb0b4469f18d99208.yaml\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing YAML files:  47%|████▋     | 1858/3977 [00:25<00:27, 75.78it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: Missing required fields in data/keno_1000/annotations/v2.0/398e43dd8fc5c17e7b4678262339566d1508d356dd5aa0b0dc329eea619ff680.yaml\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing YAML files:  52%|█████▏    | 2059/3977 [00:27<00:25, 75.05it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: Missing required fields in data/keno_1000/annotations/v2.0/edbb92a7d867ed7025f190d9cc03565bc2c920d3420fa0ffb7f1760e8d577fe0.yaml\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing YAML files:  57%|█████▋    | 2252/3977 [00:30<00:22, 76.64it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: Missing required fields in data/keno_1000/annotations/v2.0/5c706806a124a37884bb9e6f54ffeed7e0e00b0aa3d8a6ad49861437be17f9c5.yaml\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing YAML files:  63%|██████▎   | 2493/3977 [00:33<00:19, 76.02it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: Missing required fields in data/keno_1000/annotations/v2.0/09c6fbeaa5b13e07a09d908c83f0746a488a56891879c92022c5c10b8af3df8e.yaml\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing YAML files:  73%|███████▎  | 2894/3977 [00:39<00:14, 75.83it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: Missing required fields in data/keno_1000/annotations/v2.0/d0ba1a2c16d35544616d50551df73732e596365f2266b58a3350760d3f94c0ee.yaml\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing YAML files:  80%|████████  | 3191/3977 [00:43<00:10, 73.97it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: Missing required fields in data/keno_1000/annotations/v2.0/b19a1092121eae7988fe213bfd7195552b513cc34557b768dafb7bfba78e0c4c.yaml\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing YAML files:  82%|████████▏ | 3248/3977 [00:44<00:09, 74.12it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: Missing required fields in data/keno_1000/annotations/v2.0/4ea931a21f91cba7fe98bda2b214993d58ace75a16651724dd4ca981096f596b.yaml\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing YAML files:  98%|█████████▊| 3896/3977 [00:52<00:01, 75.42it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: Missing required fields in data/keno_1000/annotations/v2.0/30c79d3713b759b0b95b800e087e371bdf530b3d3e49c98fadba53ca6bab5a10.yaml\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing YAML files: 100%|██████████| 3977/3977 [00:54<00:00, 73.59it/s]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "80241d918278491096a5d8a23c18044b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter:   0%|          | 0/3968 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e2fab7e84e444f758fd9e0696873f569",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter:   0%|          | 0/3968 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "be4980a0d5b042bfaa8309906578faca",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter:   0%|          | 0/3968 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total samples: 3968\n",
      "Train: 3655\n",
      "val: 313\n",
      "Test: 0\n",
      "Dataset created successfully!\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f766eaadd23646729a15810963d7299c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Uploading the dataset shards:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9133f811e70d4353be8aebb0cb6fa3cf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating parquet from Arrow format:   0%|          | 0/4 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a1ee05ddf12a4db4861e6a3e66ad6436",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Uploading the dataset shards:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e735f404ab65455a833932f742c8b2af",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating parquet from Arrow format:   0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "adcfc8c2179f482898335afd9bec6cb4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Uploading the dataset shards:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "081c14edeb9c4902b64e6a01014a286f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating parquet from Arrow format:   0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "CommitInfo(commit_url='https://huggingface.co/datasets/jomoll/TAIX-reasoning-v2.0/commit/cf27185c624608591e2ee128ba445b3b7e830e2f', commit_message='Upload dataset', commit_description='', oid='cf27185c624608591e2ee128ba445b3b7e830e2f', pr_url=None, repo_url=RepoUrl('https://huggingface.co/datasets/jomoll/TAIX-reasoning-v2.0', endpoint='https://huggingface.co', repo_type='dataset', repo_id='jomoll/TAIX-reasoning-v2.0'), pr_revision=None, pr_num=None)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "folder_path = 'data/keno_1000/annotations/v2.0'\n",
    "dataset = process_yaml_files(folder_path)\n",
    "print(\"Dataset created successfully!\")\n",
    "# add one sample for validation and test respectivaly\n",
    "dataset[\"test\"] = dataset[\"val\"]\n",
    "# Upload to HuggingFace Hub\n",
    "dataset.push_to_hub(\"jomoll/TAIX-reasoning-v2.0\", private=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ad16b5df",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a5e57c6c63a6467b9ae68b762e640538",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/74 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2649d49ad3e2435ea10ce6c498168a49",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/23 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "17a64869bb49421b9c4387fdac335cf1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/19 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c30109cb6d254191bedd2cd2e5cf1624",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/74 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "941fbab82616444badce620e9b411169",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/23 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9068913f1dcb459093da3063fe3a4cd5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/19 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "48c18640e59243f6a53893b2a755c252",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading dataset shards:   0%|          | 0/73 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e69690629a724b668d7fee6d8b27c7e3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading dataset shards:   0%|          | 0/23 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "db2386cdb98d4cd1927deb29ec5d6783",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading dataset shards:   0%|          | 0/19 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e42088de2c9b40549595ac76fbeebd7d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8ab3602ef00e4d96ba200152c03e731a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "train-00000-of-00001.parquet:   0%|          | 0.00/3.62M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b88204210df94f1985513fd76275cb13",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split:   0%|          | 0/3655 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "64fd29c7bbf4424d826f3f38e92e7ea5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating val split:   0%|          | 0/313 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cb0eeb0da30c44aea0bf6dc35000ff76",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating test split:   0%|          | 0/313 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2df0a7c6f9e44026950ca4176d1c9cb7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/137595 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "83a3896dc5664d4e91d3e0b46fa2aaa4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter:   0%|          | 0/137595 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final dataset size: 3655\n",
      "Sample:\n",
      "UID: ebc3c8d0e455dee5118c7aedf93e2a4313639a688b4cb7b93068e54bcc705d4c\n",
      "Reasoning: [{\"Step\": {\"Description\": \"Assess the image quality.\", \"Action\": [\"I am looking at the chest X-ray i...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "00b4c0a1a12643878ed6703d2629aaf3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/74 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0c6f8c24fbf04adeb233e1f6ba5816e0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/23 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a331b720f4a140b6b566f14ed2f557cc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/19 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5e89c0f5903743cfba93ce3da223a27e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/74 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b40bb8d3f4234cb8a1ce19669a655e6b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/23 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3e96bcd2f46e41f98c980269f2a47853",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/19 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "13a07b1345134b7da76f047ad1a864f3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading dataset shards:   0%|          | 0/73 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "121db39391a54c4b8d6453358ee8a621",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading dataset shards:   0%|          | 0/23 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2884c821f26c4c108d404ed7cba2fc51",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading dataset shards:   0%|          | 0/19 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final dataset size: 313\n",
      "Sample:\n",
      "UID: b6c2215aac6e552fa54e483b8e2a66c1d6ef3b481c75125e345ad6c27807bf4f\n",
      "Reasoning: [{\"Step\": {\"Description\": \"Assess the image quality.\", \"Action\": [\"I am looking at the chest X-ray i...\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "from datasets import Dataset\n",
    "import pandas as pd\n",
    "\n",
    "def create_merged_dataset(split: str = \"train\") -> Dataset:\n",
    "    # Load both datasets\n",
    "    image_dataset = load_dataset(\"TLAIM/TAIX-Ray\", name=\"default\")[split]\n",
    "    label_dataset = load_dataset(\"jomoll/TAIX-reasoning-v2.0\", cache_dir=None)[split]\n",
    "\n",
    "    # Map UID to reasoning entry\n",
    "    uid2label = {row['UID']: row for row in label_dataset}\n",
    "\n",
    "    def enrich_with_labels(example):\n",
    "        uid = example[\"UID\"]\n",
    "        label_row = uid2label.get(uid)\n",
    "        # Add Reasoning if available, else mark as None\n",
    "        example[\"Reasoning\"] = label_row[\"Reasoning\"] if label_row else None\n",
    "        return example\n",
    "\n",
    "    # Apply label enrichment\n",
    "    merged_dataset = image_dataset.map(enrich_with_labels)\n",
    "\n",
    "    # Filter only samples with a reasoning trace\n",
    "    merged_dataset = merged_dataset.filter(lambda x: x[\"Reasoning\"] is not None)\n",
    "    print(f\"Final dataset size: {len(merged_dataset)}\")\n",
    "\n",
    "    # Sample print\n",
    "    print(\"Sample:\")\n",
    "    print(f\"UID: {merged_dataset[0]['UID']}\")\n",
    "    print(f\"Reasoning: {merged_dataset[0]['Reasoning'][:100]}...\")\n",
    "\n",
    "    return merged_dataset\n",
    "\n",
    "train_dataset = create_merged_dataset(\"train\")\n",
    "val_dataset = create_merged_dataset(\"val\")\n",
    "test_dataset = val_dataset\n",
    "\n",
    "dataset = DatasetDict({\n",
    "    'train': train_dataset,\n",
    "    'val': val_dataset,\n",
    "    'test': test_dataset\n",
    "})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7e251de4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dda8c6ea38c94a04a0d3a00eb590626e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Uploading the dataset shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e71a5292027b412fb385b84fc2863069",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1828 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "462a58240f0646149fe6afcbb76f0d2b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating parquet from Arrow format:   0%|          | 0/19 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "52a46d8384cc47828b03d4d6efc9a1bd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1827 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e3878c4ed8a349069548cc3214777002",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating parquet from Arrow format:   0%|          | 0/19 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ec9356dbad8b4f0393e40bcdee74885c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Uploading the dataset shards:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7fac7ae50b504ee7b5e7cacb03377c4c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/313 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "99b0f2d25d4545239d39d5bc55c4b988",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating parquet from Arrow format:   0%|          | 0/4 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9529006a537544149c6f6b0f8b94305a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Uploading the dataset shards:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c7fcaf490e2a4aa7ac5f0f963532b7e1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/313 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5b2f18c312094461a3fa9deef98eb8c5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating parquet from Arrow format:   0%|          | 0/4 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a5c917f30b014f0d92439cd182aed691",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "CommitInfo(commit_url='https://huggingface.co/datasets/jomoll/TAIX-reasoning-v2.1/commit/b815534b6939043528504714b81fc5da812ecfbb', commit_message='Upload dataset', commit_description='', oid='b815534b6939043528504714b81fc5da812ecfbb', pr_url=None, repo_url=RepoUrl('https://huggingface.co/datasets/jomoll/TAIX-reasoning-v2.1', endpoint='https://huggingface.co', repo_type='dataset', repo_id='jomoll/TAIX-reasoning-v2.1'), pr_revision=None, pr_num=None)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.push_to_hub(\"jomoll/TAIX-reasoning-v2.1\", private=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "48616b9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# delete all files in the data/keno_1000/annotations/v2.1 folder that have no 'Reasoning' section\n",
    "import yaml\n",
    "import os\n",
    "def delete_files_without_reasoning(folder_path: str):\n",
    "    \"\"\"Delete all YAML files that do not have a 'Reasoning' section.\"\"\"\n",
    "    yaml_files = [f for f in os.listdir(folder_path) if f.endswith('.yaml')]\n",
    "    \n",
    "    for yaml_file in yaml_files:\n",
    "        file_path = os.path.join(folder_path, yaml_file)\n",
    "        with open(file_path, 'r') as f:\n",
    "            data = yaml.safe_load(f)\n",
    "        \n",
    "        # Check if 'reasoning' section exists\n",
    "        if not data.get('reasoning', {}).get('Reasoning'):\n",
    "            print(f\"Deleting {file_path} - no 'Reasoning' section found.\")\n",
    "            os.remove(file_path)\n",
    "\n",
    "# delete all files that have 'annotator: gemini-2.5-flash-lite-preview-06-17'\n",
    "def delete_files_with_gemini_annotator(folder_path: str):\n",
    "    \"\"\"Delete all YAML files that have 'annotator: gemini-2.5-flash-lite-preview-06-17'.\"\"\"\n",
    "    yaml_files = [f for f in os.listdir(folder_path) if f.endswith('.yaml')]\n",
    "    \n",
    "    for yaml_file in yaml_files:\n",
    "        file_path = os.path.join(folder_path, yaml_file)\n",
    "        with open(file_path, 'r') as f:\n",
    "            data = yaml.safe_load(f)\n",
    "        \n",
    "        # Check if 'annotator' is 'gemini-2.5-flash-lite-preview-06-17'\n",
    "        if data.get('annotator') == 'gemini-2.5-flash-lite-preview-06-17':\n",
    "            print(f\"Deleting {file_path} - annotator is gemini-2.5-flash-lite-preview-06-17.\")\n",
    "            os.remove(file_path)\n",
    "\n",
    "\n",
    "\n",
    "# Specify the folder path\n",
    "folder_path = 'data/keno_1000/annotations/v2.2'\n",
    "delete_files_without_reasoning(folder_path)\n",
    "#delete_files_with_gemini_annotator(folder_path)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
